# çˆ¬è™«å·¥å…·ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [çˆ¬è™«æ£€æµ‹](#çˆ¬è™«æ£€æµ‹)
2. [é¢‘ç‡é™åˆ¶](#é¢‘ç‡é™åˆ¶)
3. [Robots.txt ç”Ÿæˆ](#robotstxt-ç”Ÿæˆ)
4. [çˆ¬è™«è¡Œä¸ºåˆ†æ](#çˆ¬è™«è¡Œä¸ºåˆ†æ)
5. [å®é™…åº”ç”¨ç¤ºä¾‹](#å®é™…åº”ç”¨ç¤ºä¾‹)

---

## ğŸ•·ï¸ çˆ¬è™«æ£€æµ‹

### åŸºç¡€ç”¨æ³•

```typescript
import { detectSpider, BotType } from '@repo/seo-tools/spider-detector'

// æ£€æµ‹ç”¨æˆ·ä»£ç†
const userAgent = request.headers['user-agent']
const spiderInfo = detectSpider(userAgent)

if (spiderInfo.isBot) {
  console.log('çˆ¬è™«ä¿¡æ¯:', {
    åç§°: spiderInfo.botName,
    æœç´¢å¼•æ“: spiderInfo.searchEngine,
    ç±»å‹: spiderInfo.botType,
    ç‰ˆæœ¬: spiderInfo.version,
    å¯ä¿¡: spiderInfo.isTrusted,
    ä¼˜å…ˆçº§: spiderInfo.crawlPriority
  })
}
```

### æ”¯æŒçš„çˆ¬è™«ç±»å‹

```typescript
enum BotType {
  SEARCH_ENGINE = 'search_engine',  // æœç´¢å¼•æ“ï¼ˆGoogleã€Baidu ç­‰ï¼‰
  SOCIAL_MEDIA = 'social_media',    // ç¤¾äº¤åª’ä½“ï¼ˆFacebookã€Twitter ç­‰ï¼‰
  AI_SCRAPER = 'ai_scraper',        // AI çˆ¬è™«ï¼ˆChatGPTã€Claude ç­‰ï¼‰
  SEO_TOOL = 'seo_tool',            // SEO å·¥å…·ï¼ˆAhrefsã€SEMrush ç­‰ï¼‰
  SITE_MONITOR = 'site_monitor',    // ç½‘ç«™ç›‘æ§ï¼ˆUptimeRobot ç­‰ï¼‰
  FEED_READER = 'feed_reader',      // RSS è®¢é˜…å™¨ï¼ˆFeedly ç­‰ï¼‰
  UNKNOWN = 'unknown'               // æœªçŸ¥çˆ¬è™«
}
```

### è·å–çˆ¬è™«åˆ—è¡¨

```typescript
import { getBotList, getBotStatistics } from '@repo/seo-tools/spider-detector'

// è·å–æ‰€æœ‰çˆ¬è™«
const allBots = getBotList()

// æŒ‰ç±»å‹è·å–
const searchEngineBots = getBotList(BotType.SEARCH_ENGINE)
const aiScrapers = getBotList(BotType.AI_SCRAPER)

// è·å–ç»Ÿè®¡ä¿¡æ¯
const stats = getBotStatistics()
console.log('çˆ¬è™«ç»Ÿè®¡:', {
  æ€»æ•°: stats.total,
  æŒ‰ç±»å‹: stats.byType,
  å¯ä¿¡: stats.trusted,
  ä¸å¯ä¿¡: stats.untrusted
})
```

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
{
  total: 31,
  byType: {
    search_engine: 9,
    social_media: 7,
    ai_scraper: 6,
    seo_tool: 5,
    site_monitor: 2,
    feed_reader: 1,
    unknown: 1
  },
  trusted: 29,
  untrusted: 2
}
```

---

## â±ï¸ é¢‘ç‡é™åˆ¶

### åŸºç¡€ç”¨æ³•

```typescript
import { SpiderRateLimiter, BotType } from '@repo/seo-tools/spider-detector'

// åˆ›å»ºé™åˆ¶å™¨ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
const limiter = new SpiderRateLimiter()

// æ£€æŸ¥æ˜¯å¦å…è®¸è®¿é—®
const ip = request.ip
const botType = spiderInfo.botType

if (!limiter.canVisit(ip, botType)) {
  return new Response('Too many requests', { status: 429 })
}
```

### è‡ªå®šä¹‰é…ç½®

```typescript
// åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„é™åˆ¶å™¨
const limiter = new SpiderRateLimiter({
  maxVisitsPerMinute: 20,
  maxVisitsPerHour: 300,
  burstSize: 5  // 1ç§’å†…æœ€å¤š5æ¬¡è®¿é—®
})

// ä¸ºç‰¹å®šç±»å‹è®¾ç½®é…é¢
limiter.setBotConfig(BotType.AI_SCRAPER, {
  maxVisitsPerMinute: 2,
  maxVisitsPerHour: 20,
  burstSize: 1  // ä¸¥æ ¼é™åˆ¶ AI çˆ¬è™«
})

limiter.setBotConfig(BotType.SEARCH_ENGINE, {
  maxVisitsPerMinute: 60,
  maxVisitsPerHour: 1000,
  burstSize: 20  // æœç´¢å¼•æ“ç»™äºˆæ›´é«˜é…é¢
})
```

### é»˜è®¤é…é¢ï¼ˆå†…ç½®ï¼‰

| çˆ¬è™«ç±»å‹ | æ¯åˆ†é’Ÿ | æ¯å°æ—¶ | çªå‘å¤§å° |
|---------|--------|--------|----------|
| æœç´¢å¼•æ“ | 30 | 500 | 10 |
| ç¤¾äº¤åª’ä½“ | 20 | 300 | 8 |
| AI çˆ¬è™« | 5 | 50 | 2 |
| SEO å·¥å…· | 15 | 200 | 5 |
| ç½‘ç«™ç›‘æ§ | 3 | 30 | 1 |
| Feed é˜…è¯»å™¨ | 5 | 50 | 2 |
| æœªçŸ¥çˆ¬è™« | 2 | 20 | 1 |

### è®¿é—®ç»Ÿè®¡

```typescript
// è·å–è®¿é—®ç»Ÿè®¡
const stats = limiter.getVisitStats(ip)
console.log({
  æœ€è¿‘1åˆ†é’Ÿè®¿é—®: stats.lastMinute,
  æœ€è¿‘1å°æ—¶è®¿é—®: stats.lastHour,
  æœ€åè®¿é—®æ—¶é—´: stats.lastVisit
})

// æ¸…ç†è¿‡æœŸæ•°æ®
setInterval(() => {
  limiter.cleanup()
}, 60000) // æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡

// æ¸…é™¤ç‰¹å®š IP çš„è®°å½•
limiter.clear(ip)

// æ¸…é™¤æ‰€æœ‰è®°å½•
limiter.clearAll()
```

---

## ğŸ¤– Robots.txt ç”Ÿæˆ

### åŸºç¡€ç”¨æ³•

```typescript
import { generateRobotsTxt } from '@repo/seo-tools/spider-detector'

// ç”ŸæˆåŸºç¡€ robots.txt
const robotsTxt = generateRobotsTxt({
  allowPaths: ['/', '/blog/*'],
  disallowPaths: ['/api/*', '/admin/*'],
  sitemapUrl: 'https://example.com/sitemap.xml',
  crawlDelay: 1
})
```

### SEO å‹å¥½ç‰ˆæœ¬

```typescript
import { generateSEOFriendlyRobotsTxt } from '@repo/seo-tools/spider-detector'

// ç”Ÿæˆ SEO ä¼˜åŒ–çš„ robots.txtï¼ˆæ¨èï¼‰
const robotsTxt = generateSEOFriendlyRobotsTxt(
  'https://example.com/sitemap.xml',
  true  // é˜»æ­¢ AI çˆ¬è™«
)
```

### é«˜çº§é…ç½®

```typescript
const robotsTxt = generateRobotsTxt({
  // å…è®¸çš„è·¯å¾„
  allowPaths: [
    '/',
    '/blog',
    '/blog/*',
    '/products/*',
    '/sitemap.xml'
  ],

  // ç¦æ­¢çš„è·¯å¾„
  disallowPaths: [
    '/api/*',
    '/admin/*',
    '/_next/*',
    '/private/*',
    '/*.json$',
    '/*?*utm_',     // å±è”½å¸¦ UTM å‚æ•°çš„ URL
    '/*?*session',  // å±è”½å¸¦ session çš„ URL
  ],

  // Sitemapï¼ˆå¯ä»¥æ˜¯æ•°ç»„ï¼‰
  sitemapUrl: [
    'https://example.com/sitemap.xml',
    'https://example.com/sitemap-blog.xml',
    'https://example.com/sitemap-products.xml'
  ],

  // é»˜è®¤çˆ¬å–å»¶è¿Ÿ
  crawlDelay: 1,

  // é˜»æ­¢ AI çˆ¬è™«
  blockAI: true,

  // é’ˆå¯¹ç‰¹å®šçˆ¬è™«çš„è‡ªå®šä¹‰è§„åˆ™
  customRules: {
    'Googlebot': {
      crawlDelay: 0  // Google ä¸éœ€è¦å»¶è¿Ÿ
    },
    'Baiduspider': {
      crawlDelay: 2,  // ç™¾åº¦ç¨æ…¢ä¸€ç‚¹
      disallow: ['/en/*']  // ç™¾åº¦ä¸çˆ¬è‹±æ–‡é¡µé¢
    },
    'GPTBot': {
      disallow: ['/']  // å®Œå…¨ç¦æ­¢ GPTBot
    }
  },

  // ä¸»æœºåï¼ˆå¤šåŸŸåæ—¶ä½¿ç”¨ï¼‰
  host: 'www.example.com'
})
```

### ç”Ÿæˆçš„ robots.txt ç¤ºä¾‹

```
# Robots.txt - Generated by SEO Tools
# Last updated: 2024-11-13T22:30:00.000Z

User-agent: *
Allow: /
Allow: /blog
Allow: /blog/*
Allow: /sitemap.xml
Allow: /sitemap-*.xml
Disallow: /api/*
Disallow: /admin/*
Disallow: /_next/*
Disallow: /private/*
Disallow: /*.json$
Disallow: /*?*utm_
Disallow: /*?*session
Disallow: /*?*sid
Crawl-delay: 1

# Block AI scrapers
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: Applebot-Extended
Disallow: /

# Custom rules for specific bots

User-agent: Googlebot
Crawl-delay: 0

User-agent: Baiduspider
Crawl-delay: 2

Sitemap: https://example.com/sitemap.xml
```

---

## ğŸ“Š çˆ¬è™«è¡Œä¸ºåˆ†æ

### åŸºç¡€ç”¨æ³•

```typescript
import { SpiderAnalytics } from '@repo/seo-tools/spider-detector'

const analytics = new SpiderAnalytics()

// è®°å½•çˆ¬è™«è®¿é—®
analytics.recordVisit('Googlebot', '/blog/article-1', Date.now())
analytics.recordVisit('Googlebot', '/blog/article-2', Date.now())
analytics.recordVisit('Googlebot', '/products/item-1', Date.now())

// è·å–çˆ¬è™«å‹å¥½åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰
const score = analytics.getCrawlabilityScore('Googlebot')
console.log('Googlebot å‹å¥½åº¦è¯„åˆ†:', score)

// è·å–æ‰€æœ‰çˆ¬è™«çš„ç»Ÿè®¡
const allStats = analytics.getAllStats()
console.log('æ‰€æœ‰çˆ¬è™«ç»Ÿè®¡:', allStats)
```

### å‹å¥½åº¦è¯„åˆ†ç®—æ³•

è¯„åˆ†åŸºäºä»¥ä¸‹å› ç´ ï¼š

- **è®¿é—®é¡µé¢æ•°é‡**ï¼ˆ40åˆ†ï¼‰ï¼šè®¿é—®çš„å”¯ä¸€é¡µé¢è¶Šå¤šï¼Œè¯´æ˜çˆ¬è™«è¦†ç›–è¶Šå…¨é¢
- **æ€»è®¿é—®æ¬¡æ•°**ï¼ˆ30åˆ†ï¼‰ï¼šè®¿é—®æ¬¡æ•°è¶Šå¤šï¼Œè¯´æ˜çˆ¬è™«æ´»è·ƒåº¦è¶Šé«˜
- **çˆ¬å–æ·±åº¦**ï¼ˆ30åˆ†ï¼‰ï¼šçˆ¬å–æ·±åº¦è¶Šæ·±ï¼Œè¯´æ˜ç½‘ç«™ç»“æ„å¯¹çˆ¬è™«è¶Šå‹å¥½

**ç¤ºä¾‹è¾“å‡ºï¼š**
```typescript
{
  Googlebot: {
    totalVisits: 150,
    uniquePages: 45,
    crawlabilityScore: 82
  },
  Baiduspider: {
    totalVisits: 80,
    uniquePages: 25,
    crawlabilityScore: 65
  }
}
```

---

## ğŸš€ å®é™…åº”ç”¨ç¤ºä¾‹

### Next.js API Route ä¸­ä½¿ç”¨

```typescript
// app/api/content/route.ts
import { NextRequest, NextResponse } from 'next/server'
import {
  detectSpider,
  SpiderRateLimiter,
  BotType
} from '@repo/seo-tools/spider-detector'

const limiter = new SpiderRateLimiter()

export async function GET(request: NextRequest) {
  const userAgent = request.headers.get('user-agent') || ''
  const ip = request.ip || 'unknown'

  // æ£€æµ‹çˆ¬è™«
  const spider = detectSpider(userAgent)

  if (spider.isBot) {
    console.log(`ğŸ•·ï¸ çˆ¬è™«è®¿é—®: ${spider.botName} (${spider.searchEngine})`)

    // é¢‘ç‡é™åˆ¶
    if (!limiter.canVisit(ip, spider.botType)) {
      return new NextResponse('Too Many Requests', {
        status: 429,
        headers: {
          'Retry-After': '60'
        }
      })
    }

    // é’ˆå¯¹ä¸åŒç±»å‹çš„çˆ¬è™«è¿”å›ä¸åŒå†…å®¹
    if (spider.botType === BotType.AI_SCRAPER && !spider.isTrusted) {
      // é˜»æ­¢ä¸å¯ä¿¡çš„ AI çˆ¬è™«
      return new NextResponse('Forbidden', { status: 403 })
    }
  }

  // æ­£å¸¸å¤„ç†è¯·æ±‚
  return NextResponse.json({ data: '...' })
}
```

### Express.js ä¸­é—´ä»¶

```typescript
// middleware/spider-protection.ts
import express from 'express'
import { detectSpider, SpiderRateLimiter, BotType } from '@repo/seo-tools/spider-detector'

const limiter = new SpiderRateLimiter()

export const spiderProtection = (req: express.Request, res: express.Response, next: express.NextFunction) => {
  const userAgent = req.headers['user-agent'] || ''
  const ip = req.ip || 'unknown'

  const spider = detectSpider(userAgent)

  if (spider.isBot) {
    // æ£€æŸ¥é¢‘ç‡é™åˆ¶
    if (!limiter.canVisit(ip, spider.botType)) {
      return res.status(429).json({
        error: 'Too many requests',
        retryAfter: 60
      })
    }

    // è®°å½•çˆ¬è™«è®¿é—®ï¼ˆå¯é€‰ï¼‰
    req.spiderInfo = spider
  }

  next()
}

// ä½¿ç”¨ä¸­é—´ä»¶
app.use(spiderProtection)
```

### ç”Ÿæˆ robots.txtï¼ˆNext.js App Routerï¼‰

```typescript
// app/robots.txt/route.ts
import { generateSEOFriendlyRobotsTxt } from '@repo/seo-tools/spider-detector'

export async function GET() {
  const robotsTxt = generateSEOFriendlyRobotsTxt(
    'https://example.com/sitemap.xml',
    true  // é˜»æ­¢ AI çˆ¬è™«
  )

  return new Response(robotsTxt, {
    headers: {
      'Content-Type': 'text/plain',
      'Cache-Control': 'public, max-age=3600'  // ç¼“å­˜1å°æ—¶
    }
  })
}
```

### ç»¼åˆç¤ºä¾‹ï¼šçˆ¬è™«ç›‘æ§ä»ªè¡¨ç›˜

```typescript
// app/api/admin/spider-stats/route.ts
import { getBotStatistics, SpiderAnalytics } from '@repo/seo-tools/spider-detector'

const analytics = new SpiderAnalytics()

export async function GET() {
  const botStats = getBotStatistics()
  const crawlStats = analytics.getAllStats()

  return Response.json({
    // å·²çŸ¥çˆ¬è™«ç»Ÿè®¡
    knownBots: botStats,

    // çˆ¬è™«è®¿é—®ç»Ÿè®¡
    crawlActivity: crawlStats,

    // Top çˆ¬è™«ï¼ˆæŒ‰å‹å¥½åº¦è¯„åˆ†æ’åºï¼‰
    topBots: Object.entries(crawlStats)
      .map(([name, stats]: [string, any]) => ({
        name,
        ...stats
      }))
      .sort((a, b) => b.crawlabilityScore - a.crawlabilityScore)
      .slice(0, 10)
  })
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æœç´¢å¼•æ“ä¼˜åŒ–

```typescript
// âœ… å¯¹æœç´¢å¼•æ“å‹å¥½
if (spider.botType === BotType.SEARCH_ENGINE) {
  // ä¸é™åˆ¶æœç´¢å¼•æ“çˆ¬è™«
  // è¿”å›å®Œæ•´å†…å®¹
  // æä¾›æ¸…æ™°çš„ sitemap
}
```

### 2. AI çˆ¬è™«ç®¡ç†

```typescript
// âš ï¸ AI çˆ¬è™«éœ€è¦ç‰¹æ®Šå¤„ç†
if (spider.botType === BotType.AI_SCRAPER) {
  // é€‰é¡¹ 1: å®Œå…¨é˜»æ­¢
  if (BLOCK_AI_CRAWLERS) {
    return new Response('Forbidden', { status: 403 })
  }

  // é€‰é¡¹ 2: ä¸¥æ ¼é™åˆ¶
  limiter.setBotConfig(BotType.AI_SCRAPER, {
    maxVisitsPerMinute: 1,
    maxVisitsPerHour: 10
  })
}
```

### 3. å®šæœŸæ¸…ç†

```typescript
// å®šæœŸæ¸…ç†è¿‡æœŸçš„è®¿é—®è®°å½•
setInterval(() => {
  limiter.cleanup()
}, 60000)  // æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
```

### 4. æ—¥å¿—è®°å½•

```typescript
if (spider.isBot) {
  logger.info('Spider visit', {
    botName: spider.botName,
    engine: spider.searchEngine,
    type: spider.botType,
    trusted: spider.isTrusted,
    priority: spider.crawlPriority,
    url: request.url,
    timestamp: new Date()
  })
}
```

### 5. æ¸è¿›å¼é™åˆ¶

```typescript
// æ ¹æ®çˆ¬è™«ä¼˜å…ˆçº§è®¾ç½®ä¸åŒçš„é™åˆ¶
const rateLimitConfig = {
  maxVisitsPerMinute: Math.max(10, spider.crawlPriority * 3),
  maxVisitsPerHour: Math.max(100, spider.crawlPriority * 50)
}
```

---

## ğŸ“ˆ æ€§èƒ½è€ƒè™‘

1. **å†…å­˜ä½¿ç”¨**ï¼š`SpiderRateLimiter` ä¼šåœ¨å†…å­˜ä¸­å­˜å‚¨è®¿é—®è®°å½•ï¼Œå®šæœŸè°ƒç”¨ `cleanup()` æ¸…ç†
2. **ç¼“å­˜**ï¼šçˆ¬è™«æ£€æµ‹ç»“æœå¯ä»¥ç¼“å­˜ï¼Œé¿å…é‡å¤æ£€æµ‹ç›¸åŒçš„ User-Agent
3. **å¼‚æ­¥å¤„ç†**ï¼šçˆ¬è™«ç»Ÿè®¡å’Œæ—¥å¿—å¯ä»¥å¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡ä¸»è¯·æ±‚

---

## ğŸ” å®‰å…¨å»ºè®®

1. **IP éªŒè¯**ï¼šå¯¹äºé‡è¦çˆ¬è™«ï¼ˆå¦‚ Googlebotï¼‰ï¼Œå¯ä»¥é€šè¿‡åå‘ DNS éªŒè¯çœŸå®æ€§
2. **User-Agent éªŒè¯**ï¼šä¸è¦å®Œå…¨ä¿¡ä»» User-Agentï¼Œç»“åˆ IPã€è¡Œä¸ºæ¨¡å¼ç»¼åˆåˆ¤æ–­
3. **æ¸è¿›å¼é˜»æ­¢**ï¼šå¯¹å¯ç–‘çˆ¬è™«å…ˆé™åˆ¶ï¼Œå†é˜»æ­¢ï¼Œé¿å…è¯¯ä¼¤
4. **ç›‘æ§å¼‚å¸¸**ï¼šç›‘æ§çˆ¬è™«è¡Œä¸ºï¼Œå‘ç°å¼‚å¸¸ç«‹å³å‘Šè­¦

---

## ğŸ“š æ›´å¤šèµ„æº

- [æœç´¢å¼•æ“çˆ¬è™«å®˜æ–¹æ–‡æ¡£](https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers)
- [robots.txt è§„èŒƒ](https://developers.google.com/search/docs/crawling-indexing/robots/intro)
- [AI çˆ¬è™«ç®¡ç†æŒ‡å—](https://platform.openai.com/docs/gptbot)
